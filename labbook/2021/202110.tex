\chapter[2021 October]{October 2021}

\section[2021/10/04]{Tuesday, 04 October 2021}

\subsection{PCB Assembly}

The PCB for the main robotic controller board as well as the supporting pressure sensor and \ac{LED} breakout boards were received from the JLC PCB board house. The top-view of the main controller board is shown in \FigRef{fig:empty-pcb} below.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.6\linewidth]{figures/202110/pcb-board.jpg}
	\caption{Empty PCB board as received from the JLC PCB board house.}
	\label{fig:empty-pcb}
\end{figure}

The components required to assemble the board were acquired prior to the delivery of the PCB board. Due to the close proximity of the components, the components were required to be strategically soldered in order to prevent any obstructions that prevented subsequent components from being soldered in their intended location. Due to the fine pitch (0.5mm) nature of the STM32L072RZT6 microcontroller package, a drag soldering technique was used in conjunction with a significant amount of solder flux to successfully solder the component. The board was cleaned using \ac{IPA} alcohol throughout the soldering process. The assembled PCB board is shown in \FigRef{fig:assembled-pcb} below.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.6\linewidth]{figures/202110/final-pcb.jpg}
	\caption{PCB board containing all the required components after soldering.}
	\label{fig:assembled-pcb}
\end{figure}

\pendsign

\section[2021/10/10]{Sunday, 10 October 2021}

\subsection{OpenGL}

One of the specifications for this project requires the development of a PC-based \ac{GUI} software component that allows the user to define 3D shapes to be constructed by the robot. Furthermore, the project proposal indicates that this software component must make use of graphical primitives to generate a 3D render of the shape as part of this process. To this extent, the OpenGL specification was selected as the basis for the 3D rendering component. It is noted that OpenGL is only a specification for a graphics \ac{API} and not an implementation in itself. This \ac{API} is usually implemented by the graphics card manufacturers. Furthermore it is noted that can be considered to be a state machine which is described as the graphical context. The behaviour of various OpenGL instructions depends on the state of the OpenGL context.

In order to make use of and learn OpenGL, some supporting software components were required. Firstly, \ac{GLFW} was used to provide a basic window to render OpenGL content in as well as to provide basic mechanisms of interaction with this window through computer peripherals. Secondly, as OpenGL is just a specification, there are a number of versions of drivers that implement this specification. However, the location of these drivers is usually unknown at the point of compilation ad as a result their locations need to be fetched upon execution of the OpenGL dependent program. In order to overcome this issue, \ac{GLAD} was used since it fetches the location of such drivers and makes them available for use in the OpenGL dependent program.

The foundation of rendering using OpenGL rests on the use of vertices. Specifically, a number of vertices are defined as the starting point for various objects to be rendered in 3D. These vertices are assembled into shapes such as triangles depending on the OpenGL context. Triangles were used as the primitive shape created from vertices in this project. Surfaces can be created by generating a number of these shapes adjacent to each other with varying sizes and orientations. These vertices and shapes encounter a number of processes and transformations when being converted from a description in 3D space to a collection of pixels on a 2D computer screen. There are approximately six such steps in this process which are known as the six stages of the graphics pipeline which is shown in \FigRef{fig:graphics-pipeline} below.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.7\linewidth]{figures/202110/graphics-pipeline.PNG}
	\caption{Six stages of the graphics pipeline.}
	\label{fig:graphics-pipeline}
\end{figure}

Two of the stages shown in \FigRef{fig:graphics-pipeline} require an implementation to be defined when using OpenGL, namely the vertex shader and the fragment shader. These shaders are written using \ac{GLSL} which is a language similar to C that is used to write shader programs that execute on the graphics card. In order to define the positions and orientations of the various objects to be rendered as well as how they are transformed to 2D space on the screen, a number of coordinate systems and transformation matrices are required. These are shown in \FigRef{fig:graphics-pipeline} below.

\begin{figure}[H]
	\centering
	\includegraphics[width=1\linewidth]{figures/202110/transformation-matrices.PNG}
	\caption{Transformation matrices involved in mapping the vertices from local space to screen space.}
	\label{fig:graphics-pipeline}
\end{figure}

The object is described in the local coordinate frame with the origin of the frame of reference usually located somewhere on or within the object. World space is the frame of reference that relates the position, orientation and scale of all the objects to be rendered together. The matrix that maps the local frame to the world frame is known as the model matrix. Similarly, the view of the world, usually thought of as a camera, has its own coordinate system. The world space is mapped to this view space by means of a view matrix. This defines the angle the world is seen from. Lastly, it is noted that the coordinates that are mapped to the screen need to be \ac{NDC} where all vertex values of the coordinate axes are between 0 and 1. Any vertex outside of this space will not be projected onto the screen. As such, this space is called clip space and the projection matrix is used to map from view space to clip space. However, the projection matrix can be used to define the nature of this clip space and as such can be used to control the projection perspective. 

Lastly, the viewport transform is performed to convert the 3D coordinates to 2D coordinates. The former three transforms, namely the model, view and projection matrices, are the transforms that will be manipulated to transform the vertices accordingly. Each of these matrices consist of a translation, rotation and scale sub-component of which the order of operations is important to ensure the correct transform result. By defining cubes using vertex arrays and manipulating these vertices as described above, a pyramid 3D shape that the robot could build was rendered as shown in \FigRef{fig:initial-opengl} below.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\linewidth]{figures/202110/initial-opengl-shape.PNG}
	\caption{3D render of pyramid shape generated using OpenGL within QT framework.}
	\label{fig:initial-opengl}
\end{figure}

\pendsign

\section[2021/10/14]{Thursday, 14 October 2021}

\subsection{OpenGL Camera}

The next OpenGL component that needed to be implemented after the basic 3D rendering is the camera control. The view matrix is used to map the world space to the camera space. A convenient approach to constructing the view matrix is by using a so-called LookAt matrix $\boldsymbol{V}$. The look at matrix is defined as

\begin{align}
	\boldsymbol{V}=
	\begin{bmatrix}
		R_x & R_y & R_z & 0 \\
		U_x & U_y & U_z & 0 \\
		D_x & D_y & D_z & 0 \\
		0 & 0 & 0 & 1 \\
	\end{bmatrix}
	\begin{bmatrix}
		1 & 0 & 0 & -P_x \\
		0 & 1 & 0 & -P_y \\
		0 & 0 & 1 & -P_z \\
		0 & 0 & 0 & 1 \\
	\end{bmatrix}
\end{align}

where $\boldsymbol{R}$ is the right vector, $\boldsymbol{U}$ is the up vector, $\boldsymbol{D}$ is the direction vector and $\boldsymbol{P}$ is the position vector for the camera in the world space. These vectors defining the $\boldsymbol{V}$ matrix can be obtained from 3 vectors which are much more intuitive to use for camera control. Specifically a position, target (i.e. the point in the world space the camera is pointed towards) and up vector are sufficient to derive a look up matrix. This was used as the basis to form the camera system for this project's implementation. The desired camera behaviour was defined as follows:

\begin{compactitem}
	\item The camera frame x-axis should remain parallel to the world xz plane.
	\item The camera movement about a point should always be on the surface of a dome about that point when not translating or zooming.
	\item As the camera rotates the camera should continue to look at the same focal point.
\end{compactitem}

This behaviour was successfully implemented for the camera.

\subsection{Shape Definition}

The following features were completed regarding the OpenGL 3D shape definition component:

\begin{compactitem}
	\item Camera control by means of computer peripherals was implemented. The right mouse button can be used to control the rotation of the camera about a given focal point. The middle mouse button can be used to translate the camera horizontally in the world scene.
	\item The OpenGL shape render view was integrated with the main software \ac{GUI} component.
	\item The ability for the user to insert and remove shapes from the model as well as the ability for the user to specify the position and rotation of the cube within the 3D model space.
	\item The option to load and save model files.
\end{compactitem}

\pendsign

\section[2021/10/14]{Sunday, 24 October 2021}

\subsection{Computer Vision Component}

The computer vision component of the project was one of the first components explored in depth. As such, the most recent work performed regarding this component took place on Thursday, 16 June 2021. The reader is referred back to this section as the theoretical point of reference on which further development of this component is based. As such, concepts such as the intrinsic and extrinsic matrices are not repeated. The most recent work provided a theoretical avenue to localize the cubes in 3D space without providing an implementation. Therefore, this was the focus at this stage of the project.

The extrinsic matrix requires an additional piece of information when mapping points from the image frame to the world frame as a piece of information is lost during the inverse operation. It was postulated that this information could be obtained from the cube edge. However, the cube side length differential was not found to be sufficiently large between vertical layers to make this distinction. As such, it was decided to make use of the computer vision component looking for a cube in an expected layer where the z-axis world coordinate of the cube can be provided. This is sufficient information to use the extrinsic and camera matrices in conjunction with the image points to locate the x and y coordinates of the cube in the world frame.

The camera matrix had already been formed during the checkerboard calibration process. However, the extrinsic translation and rotation matrices still needed to be created. These matrices were attained using the $\textit{solvePnP}$ function from the OpenCV library. This functions relies on points whose location in both the image frame as well as the world frame are known. Given at least four of these points, it is possible to estimate the rotation and translation matrices. However, in practice, it was found that at least five points are required for these matrices to be successfully be determined. 

In order to provide these point correspondences, fiducials were used to mark the known points in the world location. The location of these fiducials in the camera image provided the corresponding image frame coordinates. In order to ensure the rotation and translation matrices were successfully determined, six fiducials were placed at known locations in the image frame. The fiducials were structured as a square with a white border with an imaginary internal grid of 3x3 squares such that black squares correspond with a binary 0 and white squares correspond with a binary 0. In order to uniquely determine the orientation of the fiducial, the squares at coordinates $(0, 0)$, $(2, 0)$ and $(2,2)$ were assigned binary values of 0, 0 and 1 respectively. This left 6 squares which could represent $2^6=64$ unique identifiers. The fiducials themselves were detected by applying a binary and contour detection in a similar manner to the top cube faces discussed in the previous computer vision work.

The detection of both the cubes and the fiducials were extended through the addition of corner detection algorithm. The algorithm was developed from first principles and required the use of the centroid of the contour. The first corner was selected as the contour point with the greatest Euclidean distance from the centroid. Following this the contour was divided into four quadrants with the first detected corner centred in a quadrant. The furthest points from the centroid were identified in the remaining quadrants and taken as the remaining corners. Using these corners it was possible to determine the orientation of the cube top face as well as determine the homography matrix that could be used to map the fiducial to a 2D image which could be analysed further. The binary array data of the fiducial was extracted by sampling the centre pixel in each fiducial block and using this to calculate the unique fiducial identifier. The corresponding world point location is drawn from a lookup table and used in conjunction with the fiducial centroid image location to compute the rotation and translation matrices. With these matrices formed, it is possible to calculate the world point location of a detected cube given the expected Z plane. Finally in order to visualise the 3D information the computer vision system has of the scene, a bounding box of the workspace was defined in the world space and mapped to the image frame. These components, namely the fiducial detection, cube pose detection and bounding box visualisation were all integrated with the desktop software component and are shown in \FigRef{fig:computer-vision-overview} below.

\begin{figure}[H]
	\centering
	\includegraphics[width=1\linewidth]{figures/202110/computer-vision-overview.png}
	\caption{Screen capture of the desktop software component showing the fiducial detection, cube detection and bounding box visualisation aspects of the computer vision system.}
	\label{fig:computer-vision-overview}
\end{figure}

\pendsign