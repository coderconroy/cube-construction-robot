%%
%%  Department of Electrical, Electronic and Computer Engineering.
%%  EPR400/2 Final Report - Section 1.
%%  Copyright (C) 2011-2021 University of Pretoria.
%%

\section{Literature study}

%%% ---[ BEGIN REMOVE ]---
%{\slshape
%Your literature study, described in the study guide, goes here.
%}
%%% ---[ END REMOVE ]---
%
%\subsection{Testing equation numbering}
%
%\begin{equation}
%  \Delta w_{i,j} = \alpha a_i \Delta[j]
%\end{equation}
%
%\nocite{Haykin:Communication_Systems}
%\nocite{*}

\subsection{Background}

% High-level task context

\subsubsection{Overview}

The use of artificial systems to emulate tasks that humans find straightforward to perform, such as solving 2D puzzles, has been a long-standing practice since components of the solution system are often relevant in industrial applications \cite{Burdea:Solving_Jigsaw_Puzzles_by_a_Robot}. This work focuses on a similar task that involves the construction of 3D shapes using small cubes. Such a task bears similarity to those tasks in the domain of pick and place robotics with the variation that object placement is dependent on the location of previously placed objects. Existing solutions in this domain typically consist of two primary components: a computer vision system to detect and localise the object of interest as well as a robot to alter the location and orientation of the object in 3D space \cite{Sharath:Gantry_Robot_Design}.

% Overall system literature

%- Compare in terms of robot used and in terms of object manipulated?
%
%Due to the wide range of variation possible in each of the subsystems, there exists a wide range of unique system - level approaches to solving related problems
%
%\cite{Lin:Character_Cube_Stacking_Robot} 
%- Robotic and computer vision research involving cube as the object of interest. In this case, the cubes contain markings that can be used to distinguish the cubes in images.
%- Charge-coupled device (CCD) camera was used
%- Used a previously produced robot. The robot used is a mobile robot with a size comparable to that of the cubes.
%- The orientation of the cube here about all axes of rotation is of concern while the only orientation of concern in this project is rotation about the vertical axis. Furthermore, the initial orientation of the cube could be arbitrary which is not the case in this project.
%- End effector mechanism similar to that of a gripper.
%- "Through the experiment we discovered that the biggest problem that affects the finding and recognizing of the letter  blocks  is  the  targeting  frame  becomes  unclear"

\subsubsection{Robotic System}

% Mechanical component of robotic system
% - Consider dividing into manipulator, end-effector and vacuum generation literature

A robot can be viewed as the combination of two core components, namely the robotic manipulator and the end-effector. The end-effector is the physical interface between the robot and the object of interest and is referred to as a robot gripper when its purpose is to grip the object to facilitate pose manipulation. The nature of the robot gripper depends on the physical characteristics of the object of interest and as such a wide variety of grippers have been developed. These include stiff finger grippers, flexible finger grippers, magnetic grippers and vacuum grippers which are best suited for objects with a flat surface \cite{Lundstrom:Industrial_Robot_Grippers}. The function of the robotic manipulator is to alter the position and orientation of the end-effector in 3D space. Robotic manipulators are categorised by the coordinate systems used to describe their movement mechanics which includes polar, cylindrical, articulate and Cartesian coordinates \cite{Miller:Robots_and_Robotics_Principles}. Cartesian robots have the benefit that accuracy of the robot is uniform throughout the robot's work envelope.

% Types of robots

% Robotic system motor control


% Embedded robot controller

\subsubsection{Object Detection}

% Object detection

The purpose of the computer vision system is to detect the object of interest and localise it using the input image data captured from the robot's workspace, such that the robot has sufficient information to interact with the object. It is important that a distinction is made between object detection and object recognition. Object detection refers to the process of locating instances of a given object within an image while object recognition refers to the identification and classification of an object. This work is concerned with the former as the object of interest is known to be a cube. There are a wide range of approaches to the object detection problem. These can be broadly classified as being part of either the traditional computer vision domain or the deep learning domain \cite{MathWorks:Object_Recognition}. In both cases, various techniques are used to extract information from the image data in the form of features which are subsequently used to detect the objects of interest \cite{Kumar:Visual_Servoing}. 

The distinguishing factor between traditional and deep-learning domains lies in the method of feature extraction. Traditional approaches incorporate a manual feature extraction step before the data is processed further. Deep learning approaches, on the other hand, integrate this step as part of the underlying model, such as a convolutional neural network (CNN). In this sense, such models can be viewed as highly integrated structures which take images as input after minimal preprocessing and produce the object recognition information as output. A deep-learning approach has been developed to detect generic rectangular cuboid objects in everyday scenes captured from a single perspective \cite{Xiao:Localizing_3D_Cuboids}. However, the generic nature of this task requires a highly sophisticated approach to achieve reasonable success. 

% Classical image processing techniques
% - Low level feature detection
% - Contour detection
% -- Binary images vs gradient images etc
% - SIFT etc 

The best solutions to computer vision problems that arise from unconstrained environments and require a great degree of generality are almost always found in the deep learning domain. However, when the problem is sufficiently constrained, solutions based on traditional techniques often exhibit performance that is comparable or even superior to that of deep learning approaches. In such cases, traditional approaches are often preferable since, unlike deep learning approaches, they do not require a massive training data set or a large degree of computational power. In general, a feature can be considered to be a piece of information present within the image input data. Edges, corners, blobs and ridges are examples of common low-level features that are considered within the traditional computer vision domain. Feature detectors are used to locate these fragments of information in the image input data. The Canny edge detector \cite{Canny:Computational_Edge_Detection} and Harris corner detector \cite{ Harris:Corner_and_Edge_Detector} are examples of such methods which are popular for detecting edge and corner features respectively within an image.

The blurring of an image is a preprocessing step commonly employed with traditional approaches. This operation acts as a low-pass filter and filters out high-frequency noise which manifests itself as outlier pixel intensities. Improved performance of the feature detection stage is generally observed as a result. The conversion of an image to grey-scale is another common preprocessing step which reduces the complexity of subsequent operations when the color information of the image is insignificant. Thresholding is a useful method for obtaining shape-level feature information through image segmentation and is often applied following the preprocessing phase. The application of this technique to a grey-scale image results in a binary image which lends itself to further shape-level feature extraction. Since there are exposure inconsistencies that arise between images due to environmental light variability, it is usually prudent to incorporate an automatic threshold level determination mechanism when thresholding. In the ideal case, a grey image will exhibit a bimodal distribution of pixel intensities where the minima between the peaks corresponds to the ideal threshold value. However, such pixel intensity distributions are not necessarily guaranteed in most practical applications and, as a result, more robust automatic thresholding techniques have been developed, such as Otsu's method \cite{Otsu:Threshold_Selection_Method}. Existing automatic thresholding methods are usually classified as either histogram shape-based, clustering-based, entropy-based, object attribute-based, spatial or local methods \cite{Guruprasad:Overview_of_Thresholding_Methods}.

Contours are another useful shape-level feature that can be used in service of traditional approaches to object detection. The bounding outline that captures the shape of an object in an image is considered to be a contour. There are many different approaches to contour detection which, in general, can be categorised as either pixel-based, edge-based or region-based methodologies. Contours in images frequently correspond with discontinuities in grey-scale pixel intensity, particularly in the case of contours arising from luminance changes, which are detectable through the corresponding gradient magnitude information. A common approach to  extract this information is to make use of a local filter which is convolved with the image. This results in a gradient space where the greatest gradient magnitudes are indicative of potential contours. However, this method is unreliable as it usually produces discontinuous contours and, therefore, often requires supplementary high-level feature information \cite{Gong2018:Overview_of_Contour_Detection_Approaches}.

The contour detection problem is significantly simplified when the problem space is constrained to only binary images. In this case, gradient information is not required as with grey-scale images, as pixel intensity discontinuities can be determined using only adjacent pixels. Furthermore, a binary image can be interpreted as consisting of a number of connected components where a connected component is defined as a set of pixels with identical intensity values which are interconnected through either 4-pixel or 8-pixel connectivity. Within this framework, the concept of a contour can be reduced to the sequence of pixels that define the boundary between adjacent but dissimilar connected components. The advantage of such contours is that they are guaranteed to be continuous, in contrast to the grey-scale image case. The border following algorithm is a longstanding approach to the detection of these binary image contours \cite{Suzuki:Binary_Image_Topological_Structural_Analysis}. An extension to this approach exists whereby a more advanced border labelling method is employed to facilitate the extraction of topological structure information. Such information includes the hierarchical relationship between borders as well the distinction between outer and hole borders. This approach has also been adapted such that only the top-level outer borders in the hierarchy are detected which offers improved computational performance for applications that only require such information \cite{Yokoi:Binary_Image_Topological_Properties_Analysis}.

Contour detection, in conjunction with contour template matching, has been successfully used in robotic object detection and grasping applications using a single monocular camera \cite{Wei:Robotic_Object_Recognition_With_Natural_Background}. There exist a number of other feature detectors which have applicability to cube detection. For objects with straight edges, the Hough transform is a useful image processing tool that can be used to capture these edges with parameterised straight lines in 2D space which is useful to determine the orientation of the object \cite{Aggarwal:Line_Detection_Hough_Transform}. A more advanced and robust approach to determining useful features within an image involves the use of a feature descriptor which is a vector of values that describes the local region about a given image point. A number of feature descriptor algorithms have been developed such as the scale-invariant feature transform (SIFT) \cite{Lowe:Distinctive_Image_Features_from_Scale_Invariant_Keypoints}, speeded-up robust features (SURF) \cite{Bay:SURF_Speeded_Up_Robust_Features}, features from accelerated segment test (FAST) \cite{Rosten:Machine_Learning_for_High_Speed_Corner_Detection}, binary robust independent elementary features (BRIEF) \cite{Calonder:BRIEF_Binary_Robust_Independent_Elementary_Features} and finally the oriented FAST and rotated BRIEF (ORB) \cite{Rublee:ORB_Alternative_to_SIFT_or_SURF} algorithms. These features can be used to detect instances of objects within the input image data through the process of template matching. This method has been successfully applied as part of the detection process for cubes marked with alphabetical and numerical characters \cite{Lin:Character_Cube_Stacking_Robot}.

% Object and robot localisation
% - Camera particulars
% -- Extrinsic and intrinsic matrices
% -- Camera calibration techniques
% --- Checkerboard calibration
% --- Distortion estmation and correction
% - Technqiues to determine extrinsic parameters
% -- Solve PnP approach
% - Techniques to identify known point points
% -- Fiducial markers

\subsubsection{Object Localisation}

The detection of an object within an image only forms the first stage in the robot's computer vision system. In order for the robot to interact with the object of interest in the physical world, the detected object needs to be localised such that its pose with respect to the robot's coordinate system is known. The object localisation methods available for robots when only red, green and blue (RGB) image input data is available can be categorised as either monocular vision or stereo vision approaches. With the monocular vision case, only a single RGB image is available as input at each time instance while in the stereo vision case, two or more RGB images are available \cite{Liu:6DOF_Object_Localization}. The primary drawback of monocular vision approaches is the loss of depth information that arises during the projection of the 3D world onto a single 2D image. An additional piece of information, such as the size or world plane of the object, is required in order to recover the depth information. Stereo vision approaches, on the other hand, are able to recover the depth data based on the disparity between images that arises due to difference in pose of the cameras used to capture the images \cite{Azad:Stereo-based_6D_Object_Localization}. However, stereo vision approaches require more hardware and greater computational resources than monocular vision approaches. An alternative approach is to make use of a device that captures red, green, blue and depth (RGBD) data directly, such as a time-of-flight (ToF) camera or integrated binocular stereo camera. 

In order to relate the object detection information derived the from camera input data to the world frame, the pose of the camera with respect to the world coordinate system needs to be determined. This information is represented by an extrinsic camera matrix which encompasses the rotation and translation parameters of the camera's pose with respect to the world frame. The extrinsic matrix can be used to map points in the world coordinate system to the 3D camera coordinate system. In order to map points from the 3D camera coordinate system to the 2D homogeneous coordinates in the image, an intrinsic camera matrix is used \cite{Szeliski:Computer_Vision_Algorithms_and_Applications}. Intrinsic parameters describe internal properties of the camera and are based on the pinhole camera model. These include the camera's inherent principal point offset, focal length and axis skew. The skew of the sensor axes occurs as a result of the optical axis not being exactly perpendicular to the sensor plane. However, for practical purposes this parameter is often discarded. The extrinsic matrix and intrinsic matrix can be multiplied to form the projection matrix which is used to project any point in the world frame to the image frame provided that the pinhole camera model is used and no lens distortion effects are present \cite{OpenCV:Camera_Calibration}.

The intrinsic and extrinsic camera parameters need to be determined in order to make use of the pinhole camera model in practical applications. Camera calibration is used to estimate the intrinsic characteristics of the camera while camera localisation is used to estimate the extrinsic parameters of the camera. A popular approach to camera calibration involves the use of a planar pattern with known dimensions of which multiple images are captured at various different poses \cite{Zhang:A_Flexible_Camera_Calibration_Technique}. Either the pose of the planar pattern or the camera may be altered between calibration images. The application of this algorithm to a given set of such images will produce an estimate of the intrinsic parameters of the camera as well as the radial distortion of the camera. Real-world cameras have lens-induced distortion effects that are not included as part of the pinhole camera model. Radial distortion is observed when the degree to which light rays bend when incident on the lens is not consistent with the distance from the optical centre of the lens. Tangential distortion is observed when a degree of misalignment exists between the image plane and lens. These distortion effects are described by the radial and tangential distortion coefficients respectively \cite{MathWorks:Camera_Calibration}.

In order to determine the extrinsic camera matrix, the rotation and translation of the camera with respect to the world coordinate system needs to be calculated. A popular approach to this problem involves the use of \textit{n} 3D to 2D point correspondences to calculate the camera pose for six degrees of freedom (DoF). A point correspondence refers to the situation where the 3D location of a given point in the world coordinate system, as well as the corresponding 2D location in the homogeneous image frame, are known. The most general formulation of this problem requires the computation of the camera's intrinsic parameters as well as the camera's extrinsic parameters. The Direct Linear Transform (DLT) algorithm is a well-known solution to this problem when at least six point correspondences are given \cite{OpenCV:Textured_Object_Pose_Estimation}. However, this approach suffers from a degree of inaccuracy due to the need to estimate the intrinsic parameters of the camera.

If the assumption is made that the camera is calibrated, such that the intrinsic parameters of the camera are known, the problem reduces to the Perspective-\textit{n}-Point (P\textit{n}P) problem which has been deeply explored in literature. As such, a number of iterative and non-iterative solutions to the P\textit{n}P problem have been developed. The efficient P\textit{n}P (EP\textit{n}P) algorithm is a popular non-iterative approach for the case when four or more point correspondences are given. Although four point correspondences is sufficient for EP\textit{n}P to find a solution, a greater number is preferred to provide a degree of redundancy and reduce the solution's sensitivity to noise. It is also noted that the algorithm is capable of solving for the case where the points used for the point correspondences have a planar arrangement in the world coordinate system \cite{Lepetit:EPnP}.

Within the context of the P\textit{n}P problem, the first step to solve for the camera's extrinsic parameters requires the creation of a set of point correspondences. The use of fiducial markers is a popular approach to this task, notably in the augmented reality domain. In general, a marker is an object that is placed as a known point of reference in the scene that is processed by the computer vision system. A fiducial marker system consists of three core components, namely the markers, a fiducial detector and an encoding scheme. The fiducial detector typically makes use of traditional computer vision techniques and are therefore usually characterised by simple designs that are distinct within the scene \cite{Zhang:DeepTag_General_Framework_for_Fiducial_Marker_Design}. Morphological operations often form part of the traditional methods applied during the marker detection stage  \cite{Kostak:Designing_a_Simple_Fiducial_Marker}. 

The design of the marker ensures it is rotationally asymmetric while encoding a piece of information such as the fiducial identifier. Some existing fiducial families, such as ARToolKitPlus \cite{Wagner:ARToolKitPlus} and AprilTag \cite{Olson:AprilTag}, make use of black and white square grids to encode binary data where each cell represents a binary digit. In order to extract information from the markers, perspective distortion needs to be eliminated. In the case of square markers, the corners of each marker candidate are used to compute a homography that is used to remove this distortion \cite{Hirzer:Marker_Detection_for_Augmented_Reality_Applications}. Finally, a unique marker with a known location in the world coordinate system can be used to create a point correspondence through the combination of this information with its detected location in the image coordinate frame. The use of multiple markers facilitates the creation of the requisite point correspondence set.

% Computer graphics
% - Existing APIs, frameworks
% - OpenGL

\subsection{Applicability to Project}

% >> Reflection
% Brief summary of how you applied the above background (one to two pages)
% Summarises what has been learnt in the literature study and describes how you applied what you learnt from literature (and perhaps expanded what has been reported on before in the literature)

There exists a substantial amount of literature exploring the field of artificial systems and their application in various problem domains. Problems addressed by artificial system solutions in the domain of pick and place robotics are generally the most closely related to the 3D shape construction task explored in this project. It was noted that, at a system level, solutions in this domain typically consist of a computer vision system used in conjunction with a robot. This information, in addition to the implementation details of these systems, was used as a starting point to guide the system-level design in this project.

In terms of the robotic system, the general approach to the mechanical design of a robot can be partitioned into the design of two distinct components, namely the robotic manipulator and the end-effector. The literature highlights a wide variety of approaches to the design of both these mechanical facets. In both cases, the selection of a particular approach depends substantially on the the nature of the problem space. Fortunately, there is an expansive body of knowledge in terms of the strengths and weaknesses for each of the approaches for both the end-effector and the robotic manipulator with respect to a number of problem domains. As such, the literature was used as the basis for the mechanical design approach selected decisions made in this project. In addition, the machine design procedure frequently appears as a core component of the mechanical design process in similar projects. Therefore, this procedure was also incorporated into the robot design process utilised in this project.

% Discuss object detection
The detection and localisation of the constituent construction cubes by the computer vision system is an essential component of this project that is required to facilitate interaction of the robot with dropped cubes. Fortunately, there exists a vast range of literature in the computer vision field that has direct applicability to this problem. For the cube detection component of this problem, there are two broad potential solution domains, namely the deep-learning and traditional computer vision domains. Solutions in the deep-learning domains are typically best applied to problems that require a significant degree of generality while traditional solutions are best suited to constrained problems. Traditional detection solutions are based on the manual extraction of selected features from the object of interest which are subsequently used to identify instances of the object in arbitrary images. A number of methods that are useful for extracting such features were identified in this literature study and many of these were trialed in application to the cube detection problem in this project. The methods that exhibited the greatest degree of accuracy and robustness when applied were included as part of the final computer vision system implementation.

% Discuss object localisation
Once a cube has been detected within the input image data, the location and orientation of the cube needs to be determined with respect to the robot in the physical world. This relates to the problem of object localisation which has been explored extensively in literature, particularly in the augmented reality and robotic system domains. The exact nature of the solution depends on whether a monocular or stereo vision approach is used. In general, existing solutions build on the mathematical foundation provided by the pinhole camera model. This model formed the basis of describing the relationship between the robot and camera coordinate system's in this project. The intrinsic and extrinsic camera parameters outlined within this model are sufficient to facilitate the projection of points between the coordinate systems. This was taken advantage of to determine the location of cube points with respect to the robot from the corresponding points in the input image data.

In order to make practical use of the above object localisation approach, the intrinsic and extrinsic camera parameters need to be determined. The camera calibration techniques identified in literature offer an avenue to determine the intrinsic matrix of the camera and these were subsequently applied to calibrate the camera used in this project. Similarly, solutions to the P\textit{n}P problem provide a means to determine the extrinsic camera parameters and, as such, one of the identified solutions from the literature was utilised to determine the camera extrinsics in this project. Furthermore, any solution to the P\textit{n}P problem requires a set of point correspondences between the world frame and the image frame. Existing approaches make use of fiducial markers to obtain these correspondences and the same approach was applied in this project. Furthermore, a number of the papers included as part of this literature study detail a wide variety of fiducial marker design considerations. These were used to inform the design of the fiducial markers used in this project.

% Discuss differences and conclude
Overall, there have been a number of research projects into artificial systems, consisting of a computer vision system used in conjunction with a robot to perform tasks such as 2D puzzle building or generic pick and place operations. However, the specific task of constructing moderately complex 3D shapes using small cubes does not appear to have been explored. A similar project involved the use of larger cubes marked with identification artifacts to assist in the detection of the cubes \cite{Lin:Character_Cube_Stacking_Robot}. In contrast, the cubes used in this project exhibit a plain appearance and a greater degree of reflectivity due to their metallic nature. In general, the majority of the approaches only focus on the development of a particular sub-component of the robotic and computer vision system while the bulk of the system comprises of an off-the-shelf implementation adapted to support the task in question. This work aims to develop a robot in conjunction with a computer vision system with all components tailored to fulfil the 3D shape construction task.

\newpage

%% End of File.
